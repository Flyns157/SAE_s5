# -*- coding: utf-8 -*-
"""Image2Image

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1enmrTcprYyml3sAfN5DdmXWNuYh0SMxI
"""

"""
!nvidia-smi

!pip install -Uq diffusers ftfy accelerate
!pip install -Uq transformers
!pip install -Uq controlnet-aux
"""

import torch
from PIL import Image
from io import BytesIO
from matplotlib import pyplot as plt

from diffusers import (
    StableDiffusionPipeline,
    StableDiffusionImg2ImgPipeline,
    StableDiffusionInpaintPipeline,
    StableDiffusionDepth2ImgPipeline
    )

# Choose automatically cpu instead of cuda when you don't have nvidia gpu (more faster)
def choose_device():
    device = (
        "mps"
        if torch.backends.mps.is_available()
        else "cuda"
        if torch.cuda.is_available()
        else "cpu"
    )
    device

    return device

# Generate the image depends of parameters
def img2img(device, prompt, init_image, strength, num_inference_steps):
    seed = 123456

    generator = torch.Generator(device).manual_seed(seed)

    # img2img
    result_image = img2img_pipe(
    prompt = prompt,
    image = init_image, 
    generator = generator,
    strength = strength, # Between 0 and 1 (1 for maximum changes),
    num_inference_steps = num_inference_steps, # Defaults to 50 for good results
    ).images[0]

    # Save the output image
    output_image_path = "selfhost_stablediffusion_api/utils/output_image.png"
    result_image.save(output_image_path)

    # Display init and output image
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    axs[0].imshow(init_image); axs[0].set_title('Input Image')
    axs[1].imshow(result_image); axs[1].set_title('Result Image')
    plt.show()


device_available = choose_device()

from diffusers import StableDiffusionPipeline
txt2image_pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", orch_dtype=torch.float16, variant="fp16", use_safetensors=True).to(device_available)

model_id = "stabilityai/stable-diffusion-2-1-base"
img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, variant="fp16", use_safetensors=True).to(device_available)

# Put the path of the init_image
init_image_path = "selfhost_stablediffusion_api/utils/init_image.png"
init_image = Image.open(init_image_path)

img2img(device_available, "An avatar of a man. Add a sword in his hand", init_image, 1, 50)